{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "#      FULL FIXED & REFINED: Brain Tumor Classification (Colab-ready)\n",
        "# ==============================================================================\n",
        "# This script should be pasted into a Google Colab cell or saved as a .py file\n",
        "# to be run in a suitable environment.\n",
        "#\n",
        "# Main Refinements:\n",
        "#  - Deeper and more robust model head for improved accuracy and reduced false positives.\n",
        "#  - Increased training epochs with early stopping for more thorough training.\n",
        "#  - Maintained robust Grad-CAM layer detection and safe filesystem operations.\n",
        "#  - Improved comments and structure for better readability and maintainability.\n",
        "#\n",
        "\n",
        "print(\"PART 0: Authenticating with Kaggle API...\")\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import re\n",
        "import zipfile\n",
        "import glob\n",
        "import io\n",
        "import shutil\n",
        "import subprocess\n",
        "\n",
        "# Colab-specific utilities (if running in Colab)\n",
        "try:\n",
        "    from google.colab import files\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "# --- Kaggle auth (works in Colab). If running locally, place kaggle.json at ~/.kaggle/kaggle.json ---\n",
        "if IN_COLAB:\n",
        "    if not os.path.exists(os.path.expanduser(\"~/.kaggle/kaggle.json\")):\n",
        "        print(\"Please upload your 'kaggle.json' file downloaded from your Kaggle account.\")\n",
        "        uploaded = files.upload()\n",
        "        if 'kaggle.json' not in uploaded:\n",
        "            print(\"\\n'kaggle.json' not found. Please restart the runtime and upload the file to proceed.\")\n",
        "        else:\n",
        "            kaggle_dir = os.path.expanduser('~/.kaggle')\n",
        "            os.makedirs(kaggle_dir, exist_ok=True)\n",
        "            with open(os.path.join(kaggle_dir, 'kaggle.json'), 'wb') as f:\n",
        "                f.write(uploaded['kaggle.json'])\n",
        "            os.chmod(os.path.join(kaggle_dir, 'kaggle.json'), 0o600)\n",
        "            print(\"\\nKagle API credentials configured successfully.\")\n",
        "    else:\n",
        "        print(\"Kaggle API credentials already exist.\")\n",
        "else:\n",
        "    print(\"Not running in Colab. Make sure kaggle.json is available at ~/.kaggle/kaggle.json if you want to use Kaggle API.\")\n",
        "\n",
        "# ==============================================================================\n",
        "#                      PART 1: SETUP AND IMPORTS\n",
        "# ==============================================================================\n",
        "print(\"\\nPART 1: Installing (if needed) dependencies and importing modules...\")\n",
        "\n",
        "# Install packages if running in Colab or if missing\n",
        "def ensure_package(pkg_name):\n",
        "    try:\n",
        "        __import__(pkg_name)\n",
        "    except ImportError:\n",
        "        print(f\"Installing {pkg_name}...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg_name])\n",
        "\n",
        "# The following packages are required for the notebook\n",
        "for p in [\n",
        "    'kagglehub',\n",
        "    'tensorflow',\n",
        "    'plotly',\n",
        "    'seaborn',\n",
        "    'ipywidgets',\n",
        "    'opencv-python-headless',\n",
        "    'Pillow'\n",
        "]:\n",
        "    ensure_package(p)\n",
        "\n",
        "# Now imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
        "from tensorflow.keras.applications import EfficientNetV2B2, efficientnet_v2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.layers import Conv2D, SeparableConv2D, DepthwiseConv2D\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "try:\n",
        "    import ipywidgets as widgets\n",
        "    from IPython.display import display, clear_output\n",
        "except ImportError:\n",
        "    widgets = None\n",
        "\n",
        "from PIL import Image\n",
        "import kagglehub\n",
        "\n",
        "# --- Centralized Configuration ---\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "# Increased epochs for more thorough training, with EarlyStopping to prevent overfitting\n",
        "EPOCHS = 50\n",
        "MODEL_SAVE_PATH = 'best_brain_tumor_model_efficient_refined.h5'  # HDF5\n",
        "print(\"Setup complete.\")\n",
        "\n",
        "# ==============================================================================\n",
        "#                  PART 2: DATA DOWNLOAD / PREP (ADJUST IF NEEDED)\n",
        "# ==============================================================================\n",
        "print(\"\\nPART 2: Downloading/extracting dataset and preparing data pipeline...\")\n",
        "\n",
        "try:\n",
        "    dataset_path = kagglehub.dataset_download(\"masoudnickparvar/brain-tumor-mri-dataset\")\n",
        "except Exception as e:\n",
        "    print(\"Warning: kagglehub.download failed or is unavailable. If you already have the dataset, set 'data_source_dir' manually.\")\n",
        "    dataset_path = None\n",
        "\n",
        "DATA_DIR_BASE = \"brain-tumor-mri-dataset\"\n",
        "\n",
        "if dataset_path and os.path.isfile(dataset_path):\n",
        "    print(f\"Dataset downloaded as a zip file. Extracting to '{DATA_DIR_BASE}'...\")\n",
        "    with zipfile.ZipFile(dataset_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(DATA_DIR_BASE)\n",
        "    data_source_dir = DATA_DIR_BASE\n",
        "elif dataset_path and os.path.isdir(dataset_path):\n",
        "    print(f\"Dataset found as a directory at: {dataset_path}\")\n",
        "    data_source_dir = dataset_path\n",
        "else:\n",
        "    if os.path.isdir(DATA_DIR_BASE):\n",
        "        data_source_dir = DATA_DIR_BASE\n",
        "    else:\n",
        "        raise FileNotFoundError(\n",
        "            f\"Could not find dataset automatically. Please download the dataset and place/extract it at: {DATA_DIR_BASE}\"\n",
        "        )\n",
        "\n",
        "training_dir = os.path.join(data_source_dir, \"Training\")\n",
        "testing_dir = os.path.join(data_source_dir, \"Testing\")\n",
        "\n",
        "image_paths = glob.glob(os.path.join(training_dir, \"*\", \"*.jpg\"))\n",
        "class_names = sorted([d for d in os.listdir(training_dir) if os.path.isdir(os.path.join(training_dir, d))])\n",
        "label_to_index = {name: i for i, name in enumerate(class_names)}\n",
        "labels = [label_to_index[os.path.basename(os.path.dirname(p))] for p in image_paths]\n",
        "\n",
        "print(f\"\\nFound {len(image_paths)} training images belonging to {len(class_names)} classes.\")\n",
        "print(\"Class Names:\", class_names)\n",
        "\n",
        "train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
        "    image_paths, labels, test_size=0.2, random_state=42, stratify=labels\n",
        ")\n",
        "\n",
        "test_image_paths = glob.glob(os.path.join(testing_dir, \"*\", \"*.jpg\"))\n",
        "if len(test_image_paths) == 0:\n",
        "    print(\"Warning: No test images found in Testing/. Proceeding without test set.\")\n",
        "    test_labels = []\n",
        "else:\n",
        "    test_labels = [label_to_index[os.path.basename(os.path.dirname(p))] for p in test_image_paths]\n",
        "\n",
        "# --- TF Data pipeline ---\n",
        "print(\"Building tf.data pipelines...\")\n",
        "\n",
        "def load_and_preprocess_image(path, label):\n",
        "    image = tf.io.read_file(path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])\n",
        "    return image, label\n",
        "\n",
        "def augment(image, label):\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.random_flip_up_down(image)\n",
        "    image = tf.image.random_brightness(image, max_delta=0.1)\n",
        "    return image, label\n",
        "\n",
        "def build_dataset(paths, labels, is_training=True):\n",
        "    AUTOTUNE = tf.data.AUTOTUNE\n",
        "    ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
        "    ds = ds.map(load_and_preprocess_image, num_parallel_calls=AUTOTUNE)\n",
        "    if is_training:\n",
        "        ds = ds.cache()\n",
        "        ds = ds.shuffle(buffer_size=len(paths))\n",
        "        ds = ds.map(augment, num_parallel_calls=AUTOTUNE)\n",
        "    ds = ds.batch(BATCH_SIZE)\n",
        "    ds = ds.map(lambda x, y: (efficientnet_v2.preprocess_input(x), tf.one_hot(y, depth=len(class_names))),\n",
        "                num_parallel_calls=AUTOTUNE)\n",
        "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "train_ds = build_dataset(train_paths, train_labels)\n",
        "val_ds = build_dataset(val_paths, val_labels, is_training=False)\n",
        "if len(test_image_paths) > 0:\n",
        "    test_ds = build_dataset(test_image_paths, test_labels, is_training=False)\n",
        "else:\n",
        "    test_ds = None\n",
        "print(\"Data pipeline is ready.\")\n",
        "\n",
        "# ==============================================================================\n",
        "#         PART 3: MODEL BUILDING, TRAINING, & EVALUATION (REFINED)\n",
        "# ==============================================================================\n",
        "print(\"\\nPART 3: Building and training the refined model...\")\n",
        "\n",
        "def build_refined_model_architecture(num_classes):\n",
        "    base_model = EfficientNetV2B2(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "    base_model.trainable = False\n",
        "\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "    # Adding more layers for a more complex head\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.7)(x) # Increased dropout for better regularization\n",
        "\n",
        "    predictions = Dense(num_classes, activation='softmax')(x)\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    return model\n",
        "\n",
        "model = build_refined_model_architecture(len(class_names))\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=1e-3), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "callbacks = [\n",
        "    ModelCheckpoint(MODEL_SAVE_PATH, monitor='val_accuracy', save_best_only=True, mode='max', verbose=1),\n",
        "    EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True, verbose=1), # Increased patience\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6, verbose=1)\n",
        "]\n",
        "\n",
        "print(\"--- Starting Feature Extraction ---\")\n",
        "history = model.fit(train_ds, epochs=EPOCHS, validation_data=val_ds, callbacks=callbacks, verbose=1)\n",
        "\n",
        "print(\"\\n--- Starting Fine-Tuning ---\")\n",
        "model.trainable = True\n",
        "model.compile(optimizer=Adam(learning_rate=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history_fine = model.fit(train_ds, epochs=EPOCHS, validation_data=val_ds, callbacks=callbacks, verbose=1)\n",
        "\n",
        "print(\"\\n--- Evaluating Final Model on Test Set ---\")\n",
        "if os.path.exists(MODEL_SAVE_PATH):\n",
        "    best_model = load_model(MODEL_SAVE_PATH)\n",
        "    best_model.compile(loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    if test_ds is not None:\n",
        "        test_loss, test_acc = best_model.evaluate(test_ds)\n",
        "        print(f\"\\nFinal Test Accuracy: {test_acc*100:.2f}%\")\n",
        "\n",
        "        y_pred_probs = best_model.predict(test_ds)\n",
        "        y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "        y_true = np.array(test_labels)\n",
        "\n",
        "        print(\"\\nClassification Report:\")\n",
        "        print(classification_report(y_true, y_pred, target_names=class_names))\n",
        "\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.ylabel('Actual')\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"No test set available to evaluate.\")\n",
        "else:\n",
        "    print(f\"Model file {MODEL_SAVE_PATH} not found. Skipping final test evaluation.\")\n",
        "\n",
        "# ==============================================================================\n",
        "#              PART 4: INTERACTIVE UI APPLICATION (ROBUST FIX)\n",
        "# ==============================================================================\n",
        "print(\"\\nPART 4: Launching interactive analysis application (widgets)...\")\n",
        "\n",
        "def get_suggestions(predicted_class):\n",
        "    suggestions = {\n",
        "        'glioma': (\"**About Glioma:** A common tumor originating in the glial cells of the brain. It can be malignant.\\n\\n\"\n",
        "                   \"**Next Steps:**\\n\"\n",
        "                   \"1. **Consult a Neuro-oncologist:** Immediate specialist consultation is crucial.\\n\"\n",
        "                   \"2. **Advanced Imaging:** Further tests like MRI with contrast or biopsy are often needed.\\n\"\n",
        "                   \"3. **Treatment:** Typically includes surgery, radiation, and chemotherapy.\"),\n",
        "        'meningioma': (\"**About Meningioma:** A tumor from the meninges (brain's protective membranes). Most are benign.\\n\\n\"\n",
        "                       \"**Next Steps:**\\n\"\n",
        "                       \"1. **Consult a Neurologist/Neurosurgeon:** To discuss the findings.\\n\"\n",
        "                       \"2. **Observation:** A 'watch-and-wait' approach may be recommended for small, asymptomatic tumors.\\n\"\n",
        "                       \"3. **Treatment:** Surgery or radiation may be used if it's large or causes symptoms.\"),\n",
        "        'pituitary': (\"**About Pituitary Tumor:** A growth in the pituitary gland. Most are benign but can affect hormones.\\n\\n\"\n",
        "                      \"**Next Steps:**\\n\"\n",
        "                      \"1. **Consult an Endocrinologist:** A hormone specialist is key.\\n\"\n",
        "                      \"2. **Hormone Level Testing:** Blood and urine tests are needed.\\n\"\n",
        "                      \"3. **Treatment:** May include medication, surgery, or radiation.\"),\n",
        "        'notumor': (\"**No Tumor Detected:** The model did not find signs of a tumor.\\n\\n\"\n",
        "                    \"**Recommendations:**\\n\"\n",
        "                    \"- This is a positive sign, but this is not a medical diagnosis.\\n\"\n",
        "                    \"- Always consult a qualified radiologist for a definitive diagnosis.\\n\"\n",
        "                    \"- Continue with regular health check-ups.\")\n",
        "    }\n",
        "    return suggestions.get(predicted_class, \"No information available.\")\n",
        "\n",
        "class PredictionPipeline:\n",
        "    def __init__(self, model_path, class_names):\n",
        "        self.class_names = class_names\n",
        "        if not os.path.exists(model_path):\n",
        "            raise FileNotFoundError(f\"Model file '{model_path}' not found. Please train/save the model first.\")\n",
        "        self.model = load_model(model_path)\n",
        "\n",
        "        conv_types = (Conv2D, SeparableConv2D, DepthwiseConv2D)\n",
        "        last_conv_layer_name = None\n",
        "\n",
        "        for layer in reversed(self.model.layers):\n",
        "            if isinstance(layer, conv_types):\n",
        "                last_conv_layer_name = layer.name\n",
        "                break\n",
        "\n",
        "        if last_conv_layer_name is None:\n",
        "            for layer in reversed(self.model.layers):\n",
        "                if hasattr(layer, 'output_shape') and len(layer.output_shape) == 4:\n",
        "                    last_conv_layer_name = layer.name\n",
        "                    break\n",
        "\n",
        "        if last_conv_layer_name is None:\n",
        "            raise ValueError(\"Could not find a valid convolutional layer for Grad-CAM.\")\n",
        "\n",
        "        self.last_conv_layer_name = last_conv_layer_name\n",
        "        self.grad_model = Model(\n",
        "            [self.model.inputs], [self.model.get_layer(last_conv_layer_name).output, self.model.output]\n",
        "        )\n",
        "\n",
        "    def predict_and_visualize(self, img_bytes):\n",
        "        img = Image.open(io.BytesIO(img_bytes)).convert('RGB').resize((IMG_SIZE, IMG_SIZE))\n",
        "        img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "        preprocessed_img = efficientnet_v2.preprocess_input(np.expand_dims(img_array, axis=0))\n",
        "\n",
        "        prediction = self.model.predict(preprocessed_img, verbose=0)\n",
        "        predicted_class_index = int(np.argmax(prediction, axis=1)[0])\n",
        "        predicted_class_name = self.class_names[predicted_class_index]\n",
        "        confidence = float(np.max(prediction) * 100)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            conv_outputs, preds = self.grad_model(preprocessed_img)\n",
        "            tape.watch(conv_outputs)\n",
        "            loss = preds[:, predicted_class_index]\n",
        "\n",
        "        grads = tape.gradient(loss, conv_outputs)\n",
        "        if grads is None:\n",
        "            heatmap = np.zeros((conv_outputs.shape[1], conv_outputs.shape[2]))\n",
        "        else:\n",
        "            pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "            conv_outputs_np = conv_outputs[0].numpy()\n",
        "            pooled_grads_np = pooled_grads.numpy()\n",
        "            heatmap = np.dot(conv_outputs_np, pooled_grads_np)\n",
        "\n",
        "            heatmap = np.maximum(heatmap, 0)\n",
        "            if np.max(heatmap) != 0:\n",
        "                heatmap /= np.max(heatmap)\n",
        "\n",
        "        original_img_cv = np.array(img).astype(np.uint8)\n",
        "        heatmap_resized = cv2.resize(heatmap, (original_img_cv.shape[1], original_img_cv.shape[0]))\n",
        "        heatmap_uint8 = np.uint8(255 * heatmap_resized)\n",
        "        heatmap_color = cv2.applyColorMap(heatmap_uint8, cv2.COLORMAP_JET)\n",
        "        heatmap_color = cv2.cvtColor(heatmap_color, cv2.COLOR_BGR2RGB)\n",
        "        superimposed_img = cv2.addWeighted(original_img_cv, 0.6, heatmap_color, 0.4, 0)\n",
        "\n",
        "        return predicted_class_name, confidence, prediction.flatten(), superimposed_img\n",
        "\n",
        "try:\n",
        "    pipeline = PredictionPipeline(MODEL_SAVE_PATH, class_names)\n",
        "    print(f\"Grad-CAM will use layer: {pipeline.last_conv_layer_name}\")\n",
        "except Exception as e:\n",
        "    print(\"Warning: Could not instantiate PredictionPipeline:\", e)\n",
        "    pipeline = None\n",
        "\n",
        "if widgets is not None and pipeline is not None:\n",
        "    uploader = widgets.FileUpload(accept='image/*', description='Upload MRI', button_style='primary')\n",
        "    output_area = widgets.Output()\n",
        "\n",
        "    def md_bold_to_html(text):\n",
        "        return re.sub(r\"\\*\\*(.*?)\\*\\*\", r\"<b>\\1</b>\", text, flags=re.S)\n",
        "\n",
        "    def on_upload_change(change):\n",
        "        with output_area:\n",
        "            clear_output(wait=True)\n",
        "            if not change['new']:\n",
        "                return\n",
        "\n",
        "            first_key = list(change['new'].keys())[0]\n",
        "            file_info = change['new'][first_key]\n",
        "            img_bytes = file_info['content']\n",
        "\n",
        "            predicted_class, confidence, scores, gradcam_img = pipeline.predict_and_visualize(img_bytes)\n",
        "\n",
        "            display(widgets.HTML(f\"\"\"\n",
        "                <h3>Prediction: <span style='color:#007bff;'>{predicted_class.upper()}</span></h3>\n",
        "                <h4>Confidence: {confidence:.2f}%</h4>\"\"\"))\n",
        "\n",
        "            _, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
        "            ax1.imshow(Image.open(io.BytesIO(img_bytes)))\n",
        "            ax1.set_title(\"Uploaded MRI\")\n",
        "            ax1.axis('off')\n",
        "            ax2.imshow(gradcam_img)\n",
        "            ax2.set_title(\"Grad-CAM Heatmap\")\n",
        "            ax2.axis('off')\n",
        "            plt.show()\n",
        "\n",
        "            df_pred = pd.DataFrame({'Class': class_names, 'Confidence': (scores * 100).astype(float)})\n",
        "            fig = px.bar(df_pred, x='Class', y='Confidence', text=df_pred['Confidence'].apply(lambda x: f'{x:.2f}%'),\n",
        "                         title=\"Prediction Confidence Scores\")\n",
        "            fig.show()\n",
        "\n",
        "            suggestions_html = md_bold_to_html(get_suggestions(predicted_class)).replace('\\n', '<br>')\n",
        "            display(widgets.HTML(f\"\"\"\n",
        "                <div style='border-top: 2px solid #007bff; margin-top: 20px; padding-top: 10px;'>\n",
        "                <h3>Information & Recommendations</h3>\n",
        "                <p>{suggestions_html}</p></div>\"\"\"))\n",
        "\n",
        "    uploader.observe(on_upload_change, names='value')\n",
        "\n",
        "    display(widgets.VBox([\n",
        "        widgets.HTML(\"<h2>🧠 Brain Tumor Classification & Analysis</h2>\"\n",
        "                     \"<p>Upload a brain MRI scan to get an AI-powered analysis. The model will run and display results below.</p>\"\n",
        "                     \"<p style='color:red;'><b>Disclaimer:</b> This is an educational tool, not a substitute for professional medical diagnosis.</p>\"),\n",
        "        uploader,\n",
        "        output_area\n",
        "    ]))\n",
        "else:\n",
        "    print(\"ipywidgets not available or pipeline couldn't be created. Skipping interactive UI.\")\n",
        "\n",
        "print('\\nAll done. This refined model should provide better performance. For Streamlit, you can adapt the `PredictionPipeline` class and the logic within `on_upload_change` to fit into your Streamlit application script.')"
      ],
      "metadata": {
        "id": "0vE0-1RFcIBX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "80105ec968f54fc7a4bbe3f122001733",
            "0dbe8605a5614d70b52dafd3536a7ddb",
            "f34c9a5f625548d5b79bc9c0f4dc5e49",
            "e29adb5f831e4e9891a9f88741af352a",
            "efd60b4c32ec4a8bbca7c50eec16533e",
            "671d0f54b64e4b6e996dcf8ea252533f",
            "458ee5bc6fe94f539dce59001ce40e0e",
            "18cadf847a6e4c3799e4c97ae9ebbce3",
            "6fde7f6e041f4b88a15ac3eb356c9e09",
            "01e0ed08d41b4e0caafc7ad6ddfb0984"
          ]
        },
        "outputId": "76020201-17f6-4907-a9c7-561305da2ea6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PART 0: Authenticating with Kaggle API...\n",
            "Please upload your 'kaggle.json' file downloaded from your Kaggle account.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1b5dc89d-0773-404b-9146-ef45904d5528\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1b5dc89d-0773-404b-9146-ef45904d5528\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "\n",
            "Kagle API credentials configured successfully.\n",
            "\n",
            "PART 1: Installing (if needed) dependencies and importing modules...\n",
            "Installing opencv-python-headless...\n",
            "Installing Pillow...\n",
            "Setup complete.\n",
            "\n",
            "PART 2: Downloading/extracting dataset and preparing data pipeline...\n",
            "Using Colab cache for faster access to the 'brain-tumor-mri-dataset' dataset.\n",
            "Dataset found as a directory at: /kaggle/input/brain-tumor-mri-dataset\n",
            "\n",
            "Found 5712 training images belonging to 4 classes.\n",
            "Class Names: ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
            "Building tf.data pipelines...\n",
            "Data pipeline is ready.\n",
            "\n",
            "PART 3: Building and training the refined model...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/efficientnet_v2/efficientnetv2-b2_notop.h5\n",
            "\u001b[1m35839040/35839040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n",
            "--- Starting Feature Extraction ---\n",
            "Epoch 1/50\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - accuracy: 0.7105 - loss: 1.0195\n",
            "Epoch 1: val_accuracy improved from -inf to 0.87927, saving model to best_brain_tumor_model_efficient_refined.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 382ms/step - accuracy: 0.7109 - loss: 1.0177 - val_accuracy: 0.8793 - val_loss: 0.3012 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.8369 - loss: 0.4798\n",
            "Epoch 2: val_accuracy improved from 0.87927 to 0.89501, saving model to best_brain_tumor_model_efficient_refined.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 64ms/step - accuracy: 0.8370 - loss: 0.4795 - val_accuracy: 0.8950 - val_loss: 0.2614 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8549 - loss: 0.3964\n",
            "Epoch 3: val_accuracy improved from 0.89501 to 0.90726, saving model to best_brain_tumor_model_efficient_refined.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 64ms/step - accuracy: 0.8549 - loss: 0.3964 - val_accuracy: 0.9073 - val_loss: 0.2540 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8693 - loss: 0.3477\n",
            "Epoch 4: val_accuracy improved from 0.90726 to 0.91426, saving model to best_brain_tumor_model_efficient_refined.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 65ms/step - accuracy: 0.8694 - loss: 0.3478 - val_accuracy: 0.9143 - val_loss: 0.2219 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.8696 - loss: 0.3428\n",
            "Epoch 5: val_accuracy improved from 0.91426 to 0.92651, saving model to best_brain_tumor_model_efficient_refined.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 69ms/step - accuracy: 0.8696 - loss: 0.3427 - val_accuracy: 0.9265 - val_loss: 0.2026 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.8872 - loss: 0.2946\n",
            "Epoch 6: val_accuracy improved from 0.92651 to 0.93088, saving model to best_brain_tumor_model_efficient_refined.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 64ms/step - accuracy: 0.8872 - loss: 0.2947 - val_accuracy: 0.9309 - val_loss: 0.1895 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9040 - loss: 0.2757\n",
            "Epoch 7: val_accuracy did not improve from 0.93088\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 61ms/step - accuracy: 0.9039 - loss: 0.2760 - val_accuracy: 0.9265 - val_loss: 0.1769 - learning_rate: 0.0010\n",
            "Epoch 8/50\n",
            "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8889 - loss: 0.2959\n",
            "Epoch 8: val_accuracy did not improve from 0.93088\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 60ms/step - accuracy: 0.8889 - loss: 0.2959 - val_accuracy: 0.9291 - val_loss: 0.1918 - learning_rate: 0.0010\n",
            "Epoch 9/50\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.8975 - loss: 0.2582\n",
            "Epoch 9: val_accuracy did not improve from 0.93088\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 63ms/step - accuracy: 0.8975 - loss: 0.2583 - val_accuracy: 0.9291 - val_loss: 0.1851 - learning_rate: 0.0010\n",
            "Epoch 10/50\n",
            "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9013 - loss: 0.2726\n",
            "Epoch 10: val_accuracy did not improve from 0.93088\n",
            "\n",
            "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - accuracy: 0.9012 - loss: 0.2728 - val_accuracy: 0.9134 - val_loss: 0.2097 - learning_rate: 0.0010\n",
            "Epoch 11/50\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8875 - loss: 0.2828\n",
            "Epoch 11: val_accuracy improved from 0.93088 to 0.93613, saving model to best_brain_tumor_model_efficient_refined.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 65ms/step - accuracy: 0.8876 - loss: 0.2827 - val_accuracy: 0.9361 - val_loss: 0.1600 - learning_rate: 2.0000e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9145 - loss: 0.2384\n",
            "Epoch 12: val_accuracy improved from 0.93613 to 0.94226, saving model to best_brain_tumor_model_efficient_refined.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 65ms/step - accuracy: 0.9144 - loss: 0.2385 - val_accuracy: 0.9423 - val_loss: 0.1509 - learning_rate: 2.0000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9036 - loss: 0.2739\n",
            "Epoch 13: val_accuracy improved from 0.94226 to 0.94488, saving model to best_brain_tumor_model_efficient_refined.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 69ms/step - accuracy: 0.9036 - loss: 0.2736 - val_accuracy: 0.9449 - val_loss: 0.1511 - learning_rate: 2.0000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9091 - loss: 0.2347\n",
            "Epoch 14: val_accuracy did not improve from 0.94488\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 59ms/step - accuracy: 0.9092 - loss: 0.2346 - val_accuracy: 0.9440 - val_loss: 0.1549 - learning_rate: 2.0000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9254 - loss: 0.2096\n",
            "Epoch 15: val_accuracy did not improve from 0.94488\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 62ms/step - accuracy: 0.9253 - loss: 0.2098 - val_accuracy: 0.9431 - val_loss: 0.1463 - learning_rate: 2.0000e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9124 - loss: 0.2363\n",
            "Epoch 16: val_accuracy improved from 0.94488 to 0.94838, saving model to best_brain_tumor_model_efficient_refined.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 66ms/step - accuracy: 0.9125 - loss: 0.2363 - val_accuracy: 0.9484 - val_loss: 0.1394 - learning_rate: 2.0000e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9093 - loss: 0.2453\n",
            "Epoch 17: val_accuracy did not improve from 0.94838\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 66ms/step - accuracy: 0.9093 - loss: 0.2452 - val_accuracy: 0.9431 - val_loss: 0.1422 - learning_rate: 2.0000e-04\n",
            "Epoch 18/50\n",
            "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9188 - loss: 0.2237\n",
            "Epoch 18: val_accuracy did not improve from 0.94838\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 60ms/step - accuracy: 0.9188 - loss: 0.2237 - val_accuracy: 0.9405 - val_loss: 0.1511 - learning_rate: 2.0000e-04\n",
            "Epoch 19/50\n",
            "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9207 - loss: 0.2054\n",
            "Epoch 19: val_accuracy did not improve from 0.94838\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 63ms/step - accuracy: 0.9207 - loss: 0.2056 - val_accuracy: 0.9475 - val_loss: 0.1353 - learning_rate: 2.0000e-04\n",
            "Epoch 20/50\n",
            "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9286 - loss: 0.1927\n",
            "Epoch 20: val_accuracy did not improve from 0.94838\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 63ms/step - accuracy: 0.9285 - loss: 0.1928 - val_accuracy: 0.9449 - val_loss: 0.1352 - learning_rate: 2.0000e-04\n",
            "Epoch 21/50\n",
            "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9255 - loss: 0.2125\n",
            "Epoch 21: val_accuracy did not improve from 0.94838\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 64ms/step - accuracy: 0.9254 - loss: 0.2126 - val_accuracy: 0.9458 - val_loss: 0.1313 - learning_rate: 2.0000e-04\n",
            "Epoch 22/50\n",
            "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9227 - loss: 0.2090\n",
            "Epoch 22: val_accuracy improved from 0.94838 to 0.94926, saving model to best_brain_tumor_model_efficient_refined.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 63ms/step - accuracy: 0.9227 - loss: 0.2090 - val_accuracy: 0.9493 - val_loss: 0.1356 - learning_rate: 2.0000e-04\n",
            "Epoch 23/50\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9136 - loss: 0.2175\n",
            "Epoch 23: val_accuracy improved from 0.94926 to 0.95013, saving model to best_brain_tumor_model_efficient_refined.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 65ms/step - accuracy: 0.9137 - loss: 0.2174 - val_accuracy: 0.9501 - val_loss: 0.1343 - learning_rate: 2.0000e-04\n",
            "Epoch 24/50\n",
            "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9345 - loss: 0.1885\n",
            "Epoch 24: val_accuracy improved from 0.95013 to 0.95188, saving model to best_brain_tumor_model_efficient_refined.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 66ms/step - accuracy: 0.9344 - loss: 0.1889 - val_accuracy: 0.9519 - val_loss: 0.1308 - learning_rate: 2.0000e-04\n",
            "Epoch 25/50\n",
            "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9201 - loss: 0.2134\n",
            "Epoch 25: val_accuracy did not improve from 0.95188\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 65ms/step - accuracy: 0.9202 - loss: 0.2132 - val_accuracy: 0.9510 - val_loss: 0.1310 - learning_rate: 2.0000e-04\n",
            "Epoch 26/50\n",
            "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9325 - loss: 0.1773\n",
            "Epoch 26: val_accuracy improved from 0.95188 to 0.95626, saving model to best_brain_tumor_model_efficient_refined.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 64ms/step - accuracy: 0.9325 - loss: 0.1775 - val_accuracy: 0.9563 - val_loss: 0.1222 - learning_rate: 2.0000e-04\n",
            "Epoch 27/50\n",
            "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9306 - loss: 0.1801\n",
            "Epoch 27: val_accuracy did not improve from 0.95626\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 61ms/step - accuracy: 0.9305 - loss: 0.1803 - val_accuracy: 0.9519 - val_loss: 0.1300 - learning_rate: 2.0000e-04\n",
            "Epoch 28/50\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9319 - loss: 0.1851\n",
            "Epoch 28: val_accuracy did not improve from 0.95626\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 61ms/step - accuracy: 0.9318 - loss: 0.1852 - val_accuracy: 0.9519 - val_loss: 0.1328 - learning_rate: 2.0000e-04\n",
            "Epoch 29/50\n",
            "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9254 - loss: 0.1897\n",
            "Epoch 29: val_accuracy did not improve from 0.95626\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 65ms/step - accuracy: 0.9255 - loss: 0.1898 - val_accuracy: 0.9563 - val_loss: 0.1204 - learning_rate: 2.0000e-04\n",
            "Epoch 30/50\n",
            "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9356 - loss: 0.1843\n",
            "Epoch 30: val_accuracy did not improve from 0.95626\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 59ms/step - accuracy: 0.9356 - loss: 0.1842 - val_accuracy: 0.9528 - val_loss: 0.1237 - learning_rate: 2.0000e-04\n",
            "Epoch 31/50\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9333 - loss: 0.1861\n",
            "Epoch 31: val_accuracy improved from 0.95626 to 0.95713, saving model to best_brain_tumor_model_efficient_refined.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 65ms/step - accuracy: 0.9333 - loss: 0.1861 - val_accuracy: 0.9571 - val_loss: 0.1212 - learning_rate: 2.0000e-04\n",
            "Epoch 32/50\n",
            "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9282 - loss: 0.1851\n",
            "Epoch 32: val_accuracy did not improve from 0.95713\n",
            "\n",
            "Epoch 32: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 66ms/step - accuracy: 0.9283 - loss: 0.1849 - val_accuracy: 0.9519 - val_loss: 0.1234 - learning_rate: 2.0000e-04\n",
            "Epoch 33/50\n",
            "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9360 - loss: 0.1752\n",
            "Epoch 33: val_accuracy did not improve from 0.95713\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 66ms/step - accuracy: 0.9360 - loss: 0.1752 - val_accuracy: 0.9563 - val_loss: 0.1197 - learning_rate: 4.0000e-05\n",
            "Epoch 34/50\n",
            "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9346 - loss: 0.1690\n",
            "Epoch 34: val_accuracy did not improve from 0.95713\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 64ms/step - accuracy: 0.9347 - loss: 0.1690 - val_accuracy: 0.9536 - val_loss: 0.1204 - learning_rate: 4.0000e-05\n",
            "Epoch 35/50\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9444 - loss: 0.1571\n",
            "Epoch 35: val_accuracy did not improve from 0.95713\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 67ms/step - accuracy: 0.9443 - loss: 0.1572 - val_accuracy: 0.9528 - val_loss: 0.1192 - learning_rate: 4.0000e-05\n",
            "Epoch 36/50\n",
            "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9403 - loss: 0.1600\n",
            "Epoch 36: val_accuracy did not improve from 0.95713\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 62ms/step - accuracy: 0.9403 - loss: 0.1601 - val_accuracy: 0.9519 - val_loss: 0.1176 - learning_rate: 4.0000e-05\n",
            "Epoch 37/50\n",
            "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9340 - loss: 0.1691\n",
            "Epoch 37: val_accuracy did not improve from 0.95713\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 64ms/step - accuracy: 0.9340 - loss: 0.1691 - val_accuracy: 0.9510 - val_loss: 0.1182 - learning_rate: 4.0000e-05\n",
            "Epoch 38/50\n",
            "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9431 - loss: 0.1548\n",
            "Epoch 38: val_accuracy did not improve from 0.95713\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 59ms/step - accuracy: 0.9430 - loss: 0.1549 - val_accuracy: 0.9554 - val_loss: 0.1184 - learning_rate: 4.0000e-05\n",
            "Epoch 39/50\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9346 - loss: 0.1662\n",
            "Epoch 39: val_accuracy did not improve from 0.95713\n",
            "\n",
            "Epoch 39: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 61ms/step - accuracy: 0.9346 - loss: 0.1662 - val_accuracy: 0.9510 - val_loss: 0.1190 - learning_rate: 4.0000e-05\n",
            "Epoch 40/50\n",
            "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9350 - loss: 0.1690\n",
            "Epoch 40: val_accuracy did not improve from 0.95713\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 61ms/step - accuracy: 0.9351 - loss: 0.1689 - val_accuracy: 0.9536 - val_loss: 0.1178 - learning_rate: 8.0000e-06\n",
            "Epoch 41/50\n",
            "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9394 - loss: 0.1612\n",
            "Epoch 41: val_accuracy did not improve from 0.95713\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 66ms/step - accuracy: 0.9394 - loss: 0.1613 - val_accuracy: 0.9545 - val_loss: 0.1172 - learning_rate: 8.0000e-06\n",
            "Epoch 42/50\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9375 - loss: 0.1593\n",
            "Epoch 42: val_accuracy did not improve from 0.95713\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 60ms/step - accuracy: 0.9375 - loss: 0.1593 - val_accuracy: 0.9545 - val_loss: 0.1171 - learning_rate: 8.0000e-06\n",
            "Epoch 43/50\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9394 - loss: 0.1590\n",
            "Epoch 43: val_accuracy did not improve from 0.95713\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 61ms/step - accuracy: 0.9394 - loss: 0.1591 - val_accuracy: 0.9536 - val_loss: 0.1179 - learning_rate: 8.0000e-06\n",
            "Epoch 44/50\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9346 - loss: 0.1716\n",
            "Epoch 44: val_accuracy did not improve from 0.95713\n",
            "\n",
            "Epoch 44: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 62ms/step - accuracy: 0.9346 - loss: 0.1716 - val_accuracy: 0.9545 - val_loss: 0.1172 - learning_rate: 8.0000e-06\n",
            "Epoch 45/50\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9345 - loss: 0.1626\n",
            "Epoch 45: val_accuracy did not improve from 0.95713\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 61ms/step - accuracy: 0.9345 - loss: 0.1626 - val_accuracy: 0.9536 - val_loss: 0.1176 - learning_rate: 1.6000e-06\n",
            "Epoch 46/50\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9420 - loss: 0.1612\n",
            "Epoch 46: val_accuracy did not improve from 0.95713\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 60ms/step - accuracy: 0.9420 - loss: 0.1612 - val_accuracy: 0.9536 - val_loss: 0.1177 - learning_rate: 1.6000e-06\n",
            "Epoch 47/50\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9391 - loss: 0.1617\n",
            "Epoch 47: val_accuracy did not improve from 0.95713\n",
            "\n",
            "Epoch 47: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 61ms/step - accuracy: 0.9391 - loss: 0.1617 - val_accuracy: 0.9536 - val_loss: 0.1173 - learning_rate: 1.6000e-06\n",
            "Epoch 48/50\n",
            "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9466 - loss: 0.1482\n",
            "Epoch 48: val_accuracy did not improve from 0.95713\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 64ms/step - accuracy: 0.9465 - loss: 0.1485 - val_accuracy: 0.9536 - val_loss: 0.1177 - learning_rate: 1.0000e-06\n",
            "Epoch 49/50\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9373 - loss: 0.1699\n",
            "Epoch 49: val_accuracy did not improve from 0.95713\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 59ms/step - accuracy: 0.9374 - loss: 0.1698 - val_accuracy: 0.9528 - val_loss: 0.1178 - learning_rate: 1.0000e-06\n",
            "Epoch 49: early stopping\n",
            "Restoring model weights from the end of the best epoch: 42.\n",
            "\n",
            "--- Starting Fine-Tuning ---\n",
            "Epoch 1/50\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.9327 - loss: 0.1887\n",
            "Epoch 1: val_accuracy did not improve from 0.95713\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 255ms/step - accuracy: 0.9328 - loss: 0.1886 - val_accuracy: 0.9545 - val_loss: 0.1168 - learning_rate: 1.0000e-05\n",
            "Epoch 2/50\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9366 - loss: 0.1745\n",
            "Epoch 2: val_accuracy did not improve from 0.95713\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 61ms/step - accuracy: 0.9366 - loss: 0.1745 - val_accuracy: 0.9554 - val_loss: 0.1169 - learning_rate: 1.0000e-05\n",
            "Epoch 3/50\n",
            "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9416 - loss: 0.1545\n",
            "Epoch 3: val_accuracy did not improve from 0.95713\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 59ms/step - accuracy: 0.9415 - loss: 0.1547 - val_accuracy: 0.9536 - val_loss: 0.1174 - learning_rate: 1.0000e-05\n",
            "Epoch 4/50\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9441 - loss: 0.1593\n",
            "Epoch 4: val_accuracy did not improve from 0.95713\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 61ms/step - accuracy: 0.9441 - loss: 0.1594 - val_accuracy: 0.9545 - val_loss: 0.1167 - learning_rate: 1.0000e-05\n",
            "Epoch 5/50\n",
            "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9391 - loss: 0.1660\n",
            "Epoch 5: val_accuracy did not improve from 0.95713\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 62ms/step - accuracy: 0.9390 - loss: 0.1660 - val_accuracy: 0.9563 - val_loss: 0.1156 - learning_rate: 1.0000e-05\n",
            "Epoch 6/50\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9407 - loss: 0.1642\n",
            "Epoch 6: val_accuracy did not improve from 0.95713\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 64ms/step - accuracy: 0.9407 - loss: 0.1641 - val_accuracy: 0.9554 - val_loss: 0.1159 - learning_rate: 1.0000e-05\n",
            "Epoch 7/50\n",
            "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9402 - loss: 0.1602\n",
            "Epoch 7: val_accuracy did not improve from 0.95713\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 60ms/step - accuracy: 0.9402 - loss: 0.1601 - val_accuracy: 0.9554 - val_loss: 0.1155 - learning_rate: 1.0000e-05\n",
            "Epoch 8/50\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9422 - loss: 0.1573\n",
            "Epoch 8: val_accuracy did not improve from 0.95713\n",
            "\n",
            "Epoch 8: ReduceLROnPlateau reducing learning rate to 1.9999999494757505e-06.\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 60ms/step - accuracy: 0.9422 - loss: 0.1573 - val_accuracy: 0.9554 - val_loss: 0.1157 - learning_rate: 1.0000e-05\n",
            "Epoch 9/50\n",
            "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9320 - loss: 0.1755\n",
            "Epoch 9: val_accuracy did not improve from 0.95713\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 61ms/step - accuracy: 0.9321 - loss: 0.1754 - val_accuracy: 0.9554 - val_loss: 0.1157 - learning_rate: 2.0000e-06\n",
            "Epoch 10/50\n",
            "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9346 - loss: 0.1701\n",
            "Epoch 10: val_accuracy did not improve from 0.95713\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 64ms/step - accuracy: 0.9347 - loss: 0.1700 - val_accuracy: 0.9545 - val_loss: 0.1155 - learning_rate: 2.0000e-06\n",
            "Epoch 11/50\n",
            "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9380 - loss: 0.1642\n",
            "Epoch 11: val_accuracy did not improve from 0.95713\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 60ms/step - accuracy: 0.9380 - loss: 0.1642 - val_accuracy: 0.9545 - val_loss: 0.1155 - learning_rate: 2.0000e-06\n",
            "Epoch 12/50\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9401 - loss: 0.1617\n",
            "Epoch 12: val_accuracy did not improve from 0.95713\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 61ms/step - accuracy: 0.9401 - loss: 0.1617 - val_accuracy: 0.9545 - val_loss: 0.1155 - learning_rate: 2.0000e-06\n",
            "Epoch 13/50\n",
            "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9190 - loss: 0.2044\n",
            "Epoch 13: val_accuracy did not improve from 0.95713\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 64ms/step - accuracy: 0.9191 - loss: 0.2040 - val_accuracy: 0.9545 - val_loss: 0.1154 - learning_rate: 2.0000e-06\n",
            "Epoch 14/50\n",
            "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9322 - loss: 0.1678\n",
            "Epoch 14: val_accuracy did not improve from 0.95713\n",
            "\n",
            "Epoch 14: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 60ms/step - accuracy: 0.9323 - loss: 0.1678 - val_accuracy: 0.9554 - val_loss: 0.1155 - learning_rate: 2.0000e-06\n",
            "Epoch 15/50\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9440 - loss: 0.1594\n",
            "Epoch 15: val_accuracy did not improve from 0.95713\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 61ms/step - accuracy: 0.9440 - loss: 0.1594 - val_accuracy: 0.9545 - val_loss: 0.1156 - learning_rate: 1.0000e-06\n",
            "Epoch 16/50\n",
            "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9372 - loss: 0.1720\n",
            "Epoch 16: val_accuracy did not improve from 0.95713\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 62ms/step - accuracy: 0.9372 - loss: 0.1720 - val_accuracy: 0.9554 - val_loss: 0.1157 - learning_rate: 1.0000e-06\n",
            "Epoch 17/50\n",
            "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9438 - loss: 0.1633\n",
            "Epoch 17: val_accuracy did not improve from 0.95713\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 64ms/step - accuracy: 0.9438 - loss: 0.1633 - val_accuracy: 0.9545 - val_loss: 0.1156 - learning_rate: 1.0000e-06\n",
            "Epoch 18/50\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9331 - loss: 0.1747\n",
            "Epoch 18: val_accuracy did not improve from 0.95713\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 61ms/step - accuracy: 0.9331 - loss: 0.1747 - val_accuracy: 0.9554 - val_loss: 0.1151 - learning_rate: 1.0000e-06\n",
            "Epoch 19/50\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9359 - loss: 0.1691\n",
            "Epoch 19: val_accuracy did not improve from 0.95713\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 67ms/step - accuracy: 0.9359 - loss: 0.1692 - val_accuracy: 0.9545 - val_loss: 0.1153 - learning_rate: 1.0000e-06\n",
            "Epoch 20/50\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9364 - loss: 0.1795\n",
            "Epoch 20: val_accuracy did not improve from 0.95713\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 66ms/step - accuracy: 0.9364 - loss: 0.1795 - val_accuracy: 0.9563 - val_loss: 0.1155 - learning_rate: 1.0000e-06\n",
            "Epoch 21/50\n",
            "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9383 - loss: 0.1644\n",
            "Epoch 21: val_accuracy did not improve from 0.95713\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 66ms/step - accuracy: 0.9383 - loss: 0.1644 - val_accuracy: 0.9554 - val_loss: 0.1151 - learning_rate: 1.0000e-06\n",
            "Epoch 22/50\n",
            "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9393 - loss: 0.1549\n",
            "Epoch 22: val_accuracy did not improve from 0.95713\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 61ms/step - accuracy: 0.9393 - loss: 0.1550 - val_accuracy: 0.9554 - val_loss: 0.1150 - learning_rate: 1.0000e-06\n",
            "Epoch 23/50\n",
            "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9428 - loss: 0.1539\n",
            "Epoch 23: val_accuracy did not improve from 0.95713\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 62ms/step - accuracy: 0.9428 - loss: 0.1539 - val_accuracy: 0.9563 - val_loss: 0.1154 - learning_rate: 1.0000e-06\n",
            "Epoch 24/50\n",
            "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9378 - loss: 0.1653\n",
            "Epoch 24: val_accuracy did not improve from 0.95713\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 61ms/step - accuracy: 0.9378 - loss: 0.1652 - val_accuracy: 0.9545 - val_loss: 0.1156 - learning_rate: 1.0000e-06\n",
            "Epoch 25/50\n",
            "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9376 - loss: 0.1694\n",
            "Epoch 25: val_accuracy did not improve from 0.95713\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 66ms/step - accuracy: 0.9376 - loss: 0.1695 - val_accuracy: 0.9545 - val_loss: 0.1156 - learning_rate: 1.0000e-06\n",
            "Epoch 26/50\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9389 - loss: 0.1584\n",
            "Epoch 26: val_accuracy did not improve from 0.95713\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 61ms/step - accuracy: 0.9389 - loss: 0.1584 - val_accuracy: 0.9563 - val_loss: 0.1151 - learning_rate: 1.0000e-06\n",
            "Epoch 27/50\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9399 - loss: 0.1571\n",
            "Epoch 27: val_accuracy did not improve from 0.95713\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 62ms/step - accuracy: 0.9399 - loss: 0.1571 - val_accuracy: 0.9545 - val_loss: 0.1150 - learning_rate: 1.0000e-06\n",
            "Epoch 28/50\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9354 - loss: 0.1573\n",
            "Epoch 28: val_accuracy did not improve from 0.95713\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 64ms/step - accuracy: 0.9354 - loss: 0.1574 - val_accuracy: 0.9554 - val_loss: 0.1149 - learning_rate: 1.0000e-06\n",
            "Epoch 29/50\n",
            "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9294 - loss: 0.1749\n",
            "Epoch 29: val_accuracy did not improve from 0.95713\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 63ms/step - accuracy: 0.9295 - loss: 0.1747 - val_accuracy: 0.9563 - val_loss: 0.1154 - learning_rate: 1.0000e-06\n",
            "Epoch 30/50\n",
            "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9387 - loss: 0.1628\n",
            "Epoch 30: val_accuracy did not improve from 0.95713\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 59ms/step - accuracy: 0.9387 - loss: 0.1628 - val_accuracy: 0.9554 - val_loss: 0.1154 - learning_rate: 1.0000e-06\n",
            "Epoch 31/50\n",
            "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9358 - loss: 0.1601\n",
            "Epoch 31: val_accuracy did not improve from 0.95713\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 59ms/step - accuracy: 0.9359 - loss: 0.1601 - val_accuracy: 0.9545 - val_loss: 0.1158 - learning_rate: 1.0000e-06\n",
            "Epoch 32/50\n",
            "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9306 - loss: 0.1751\n",
            "Epoch 32: val_accuracy did not improve from 0.95713\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 61ms/step - accuracy: 0.9306 - loss: 0.1750 - val_accuracy: 0.9554 - val_loss: 0.1150 - learning_rate: 1.0000e-06\n",
            "Epoch 33/50\n",
            "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9377 - loss: 0.1643\n",
            "Epoch 33: val_accuracy did not improve from 0.95713\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 63ms/step - accuracy: 0.9377 - loss: 0.1643 - val_accuracy: 0.9554 - val_loss: 0.1154 - learning_rate: 1.0000e-06\n",
            "Epoch 34/50\n",
            "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9357 - loss: 0.1694\n",
            "Epoch 34: val_accuracy did not improve from 0.95713\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 64ms/step - accuracy: 0.9358 - loss: 0.1693 - val_accuracy: 0.9554 - val_loss: 0.1155 - learning_rate: 1.0000e-06\n",
            "Epoch 35/50\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9315 - loss: 0.1789\n",
            "Epoch 35: val_accuracy did not improve from 0.95713\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 60ms/step - accuracy: 0.9315 - loss: 0.1789 - val_accuracy: 0.9563 - val_loss: 0.1149 - learning_rate: 1.0000e-06\n",
            "Epoch 36/50\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9404 - loss: 0.1585\n",
            "Epoch 36: val_accuracy did not improve from 0.95713\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 61ms/step - accuracy: 0.9404 - loss: 0.1584 - val_accuracy: 0.9545 - val_loss: 0.1156 - learning_rate: 1.0000e-06\n",
            "Epoch 37/50\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9308 - loss: 0.1862\n",
            "Epoch 37: val_accuracy did not improve from 0.95713\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 62ms/step - accuracy: 0.9308 - loss: 0.1861 - val_accuracy: 0.9545 - val_loss: 0.1155 - learning_rate: 1.0000e-06\n",
            "Epoch 38/50\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9443 - loss: 0.1496\n",
            "Epoch 38: val_accuracy did not improve from 0.95713\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 63ms/step - accuracy: 0.9443 - loss: 0.1497 - val_accuracy: 0.9545 - val_loss: 0.1155 - learning_rate: 1.0000e-06\n",
            "Epoch 39/50\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9367 - loss: 0.1657\n",
            "Epoch 39: val_accuracy did not improve from 0.95713\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 60ms/step - accuracy: 0.9368 - loss: 0.1657 - val_accuracy: 0.9554 - val_loss: 0.1149 - learning_rate: 1.0000e-06\n",
            "Epoch 40/50\n",
            "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9410 - loss: 0.1739\n",
            "Epoch 40: val_accuracy did not improve from 0.95713\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 61ms/step - accuracy: 0.9410 - loss: 0.1738 - val_accuracy: 0.9554 - val_loss: 0.1152 - learning_rate: 1.0000e-06\n",
            "Epoch 41/50\n",
            "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9384 - loss: 0.1712\n",
            "Epoch 41: val_accuracy did not improve from 0.95713\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 64ms/step - accuracy: 0.9384 - loss: 0.1711 - val_accuracy: 0.9554 - val_loss: 0.1151 - learning_rate: 1.0000e-06\n",
            "Epoch 42/50\n",
            "\u001b[1m142/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9469 - loss: 0.1408\n",
            "Epoch 42: val_accuracy did not improve from 0.95713\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - accuracy: 0.9468 - loss: 0.1409 - val_accuracy: 0.9545 - val_loss: 0.1156 - learning_rate: 1.0000e-06\n",
            "Epoch 42: early stopping\n",
            "Restoring model weights from the end of the best epoch: 35.\n",
            "\n",
            "--- Evaluating Final Model on Test Set ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 355ms/step - accuracy: 0.9740 - loss: 0.0724\n",
            "\n",
            "Final Test Accuracy: 94.81%\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 240ms/step\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      glioma       0.98      0.87      0.92       300\n",
            "  meningioma       0.88      0.93      0.90       306\n",
            "     notumor       0.98      0.99      0.99       405\n",
            "   pituitary       0.95      0.98      0.97       300\n",
            "\n",
            "    accuracy                           0.95      1311\n",
            "   macro avg       0.95      0.94      0.94      1311\n",
            "weighted avg       0.95      0.95      0.95      1311\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "PART 4: Launching interactive analysis application (widgets)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grad-CAM will use layer: top_conv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value=\"<h2>🧠 Brain Tumor Classification & Analysis</h2><p>Upload a brain MRI scan to get a…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "80105ec968f54fc7a4bbe3f122001733"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "All done. This refined model should provide better performance. For Streamlit, you can adapt the `PredictionPipeline` class and the logic within `on_upload_change` to fit into your Streamlit application script.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "80105ec968f54fc7a4bbe3f122001733": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0dbe8605a5614d70b52dafd3536a7ddb",
              "IPY_MODEL_f34c9a5f625548d5b79bc9c0f4dc5e49",
              "IPY_MODEL_e29adb5f831e4e9891a9f88741af352a"
            ],
            "layout": "IPY_MODEL_efd60b4c32ec4a8bbca7c50eec16533e"
          }
        },
        "0dbe8605a5614d70b52dafd3536a7ddb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_671d0f54b64e4b6e996dcf8ea252533f",
            "placeholder": "​",
            "style": "IPY_MODEL_458ee5bc6fe94f539dce59001ce40e0e",
            "value": "<h2>🧠 Brain Tumor Classification & Analysis</h2><p>Upload a brain MRI scan to get an AI-powered analysis. The model will run and display results below.</p><p style='color:red;'><b>Disclaimer:</b> This is an educational tool, not a substitute for professional medical diagnosis.</p>"
          }
        },
        "f34c9a5f625548d5b79bc9c0f4dc5e49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FileUploadModel",
          "model_module_version": "1.5.0",
          "state": {
            "_counter": 0,
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FileUploadModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "FileUploadView",
            "accept": "image/*",
            "button_style": "primary",
            "data": [],
            "description": "Upload MRI",
            "description_tooltip": null,
            "disabled": false,
            "error": "",
            "icon": "upload",
            "layout": "IPY_MODEL_18cadf847a6e4c3799e4c97ae9ebbce3",
            "metadata": [],
            "multiple": false,
            "style": "IPY_MODEL_6fde7f6e041f4b88a15ac3eb356c9e09"
          }
        },
        "e29adb5f831e4e9891a9f88741af352a": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_01e0ed08d41b4e0caafc7ad6ddfb0984",
            "msg_id": "",
            "outputs": []
          }
        },
        "efd60b4c32ec4a8bbca7c50eec16533e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "671d0f54b64e4b6e996dcf8ea252533f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "458ee5bc6fe94f539dce59001ce40e0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "18cadf847a6e4c3799e4c97ae9ebbce3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6fde7f6e041f4b88a15ac3eb356c9e09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "01e0ed08d41b4e0caafc7ad6ddfb0984": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}